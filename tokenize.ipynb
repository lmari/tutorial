{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un'esplorazione della tokenizzazione\n",
    "\n",
    "Luca Mari, gennaio 2025  \n",
    "\n",
    "Quest'opera è distribuita con <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0\" target=\"_blank\">Licenza Creative Commons Attribuzione - Non commerciale - Condividi allo stesso modo 4.0 Internazionale</a>.  \n",
    "<img src=\"https://creativecommons.it/chapterIT/wp-content/uploads/2021/01/by-nc-sa.eu_.png\" width=\"100\">\n",
    "\n",
    "**Obiettivo**: comprendere la logica della \"tokenizzazione\", il processo con cui un testo viene trasformato in una successione di elementi linguistici elementari (\"token\").  \n",
    "**Precompetenze**: basi di Python.\n",
    "\n",
    "> Per eseguire questo notebook, supponiamo con VSCode, occorre:\n",
    "> * installare un interprete Python\n",
    "> * scaricare da https://code.visualstudio.com/download e installare VSCode\n",
    "> * eseguire VSCode e attivare le estensioni per Python e Jupyter\n",
    "> * ancora in VSCode:\n",
    ">     * creare una cartella di lavoro e renderla la cartella corrente\n",
    ">     * copiare nella cartella i file di questa attività: [tokenize.ipynb](tokenize.ipynb), [tokenizeutils.py](tokenizeutils.py)]\n",
    ">     * aprire il notebook `tokenize.ipynb`\n",
    ">     * creare un ambiente virtuale locale Python (Select Kernel | Python Environments | Create Python Environment | Venv, e scegliere un interprete Python):\n",
    ">     * installare i moduli Python richiesti, eseguendo dal terminale:  \n",
    ">         `pip install torch transformers colorama python-docx`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una prima elaborazione di un testo dato -- una frase, un paragrafo, un documento, ... -- è spesso la sua trasformazione in una successione di elementi linguistici elementari (\"token\"), tratti da un vocabolario dato.  \n",
    "Per mostrare un esempio di questo processo di tokenizzazione, per prima cosa importiamo il modulo che contiene le funzioni per consentire un accesso \"di alto livello\" al modello pre-addestrato che opererà come tokenizzatore, usando in questo caso una versione pre-addestrata e fine tuned, su testi in italiano, di `BERT`, che è un transformer \"open\" (https://it.wikipedia.org/wiki/BERT) ed eseguibile anche localmente (alla prima esecuzione sarà dunque necessario attendere che il modello sia scaricato dal sito di Hugging Face: è un file di circa 400 MB che viene copiato in una cache locale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il tokenizzatore ha un vocabolario di 31102 token che riconosce.\n"
     ]
    }
   ],
   "source": [
    "from tokenizeutils import Model, colorize\n",
    "\n",
    "model = Model('dbmdz/bert-base-italian-xxl-cased', True)\n",
    "\n",
    "print(f\"Il tokenizzatore ha un vocabolario di {model.vocab_size} token che riconosce.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data una frase, chiediamo al tokenizzatore di trasformarla nella lista dei suoi token, ognuno dei quali corrispondente a un identificatore numerico univoco nel vocabolario.  \n",
    "Può accadere ovviamente che una parola non sia presente nel vocabolario, cioè non sia riconosciuta come un token: in tal caso il tokenizzatore la separa in componenti, ognuno dei quali sia invece un token (per rendere evidente questa separazione, il tokenizzatore aggiunge la stringa \"##\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Il testo 'La bellezza salverà il mondo.' \n",
      "è segmentato nella lista di token:\n",
      "['La', 'bellezza', 'salve', '##rà', 'il', 'mondo', '.']\n",
      "corrispondente alla lista di identificatori numerici:\n",
      "[309, 6108, 7323, 993, 162, 1015, 697]\n"
     ]
    }
   ],
   "source": [
    "text = \"La bellezza salverà il mondo.\"\n",
    "tokens = model.tokenizer.tokenize(text)\n",
    "token_ids = model.tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f\"\\nIl testo '{text}' \\nè segmentato nella lista di token:\")\n",
    "print(tokens)\n",
    "print(\"corrispondente alla lista di identificatori numerici:\")\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternando i colori, il risultato della tokenizzazione è forse ancora più chiaro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m La\u001b[34m bellezza\u001b[31m salve\u001b[34m rà\u001b[31m il\u001b[34m mondo\u001b[31m .\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(colorize(tokens, clean=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poiché la funzione che associa un identificatore numerico a ogni token è invertibile, è sempre possibile ricostruire la frase di partenza dalla successione degli identificatori numerici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La bellezza salverà il mondo.\n"
     ]
    }
   ],
   "source": [
    "print(model.tokenizer.decode(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per curiosità, possiamo facilmente esplorare il contenuto del vocabolario, elencando alcuni dei token che contiene: è chiaro che non c'è una logica chiara nell'ordine con cui i token compaiono. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dichiarato opportuno ordini arrivo lasci SP Può sogno bellezza ##OD\n",
      "##scritto Zo vicin impegno lavorato conformemente ##ttorno Ancora faceva esplo\n",
      "magari Max ##tria produttore ricordare ##mentato dottor Slo quota resti\n",
      "presentazione 92 é ##nno privata UN accol strade Air collegamento\n",
      "##isca Antonio Dan arma offerta spettacolo ##ustralia ##uori occidentale ##74\n",
      "Ricor My fla ##-200 custo Belgio ##.12. Barcellona ##autorità motori\n",
      "assicurare ##esercizio ##bio sport ##95 bol frattempo deten ##cenze ##TER\n",
      "##uogo disciplina numeri ##teriormente ##gelo ottimo registrato ##esame ricord selezione\n",
      "##inione consegui ##ds ##tentri conosco tradizionale trasferimento qualifica interni ##derci\n",
      "rea proposto Natura ##monian adulti World ##glione ##onale ##ffo ##Com\n"
     ]
    }
   ],
   "source": [
    "for i in range(6100, 6200):\n",
    "    print(model.tokenizer.decode([i]), end=('\\n' if i % 10 == 9 else ' '))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
