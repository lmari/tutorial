{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un'esplorazione della tokenizzazione\n",
    "\n",
    "Luca Mari, gennaio 2025  \n",
    "\n",
    "Quest'opera è distribuita con <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0\" target=\"_blank\">Licenza Creative Commons Attribuzione - Non commerciale - Condividi allo stesso modo 4.0 Internazionale</a>.  \n",
    "<img src=\"https://creativecommons.it/chapterIT/wp-content/uploads/2021/01/by-nc-sa.eu_.png\" width=\"100\">\n",
    "\n",
    "**Obiettivo**: comprendere la logica della \"tokenizzazione\", il processo con cui un testo viene trasformato in una successione di elementi linguistici elementari (\"token\").  \n",
    "**Precompetenze**: basi di Python.\n",
    "\n",
    "> Per eseguire questo notebook, supponiamo con VSCode, occorre:\n",
    "> * installare un interprete Python\n",
    "> * scaricare da https://code.visualstudio.com/download e installare VSCode\n",
    "> * eseguire VSCode e attivare le estensioni per Python e Jupyter\n",
    "> * ancora in VSCode:\n",
    ">     * creare una cartella di lavoro e renderla la cartella corrente\n",
    ">     * copiare nella cartella i file di questa attività: [tokenize.ipynb](tokenize.ipynb), [tokenizeutils.py](tokenizeutils.py)]\n",
    ">     * aprire il notebook `tokenize.ipynb`\n",
    ">     * creare un ambiente virtuale locale Python (Select Kernel | Python Environments | Create Python Environment | Venv, e scegliere un interprete Python):\n",
    ">     * installare i moduli Python richiesti, eseguendo dal terminale:  \n",
    ">         `pip install torch transformers colorama python-docx`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una prima elaborazione di un testo dato -- una frase, un paragrafo, un documento, ... -- è spesso la sua trasformazione in una successione di elementi linguistici elementari (\"token\"), tratti da un vocabolario dato.  \n",
    "Per mostrare un esempio di questo processo di tokenizzazione, per prima cosa importiamo il modulo che contiene le funzioni per consentire un accesso \"di alto livello\" al modello pre-addestrato che opererà come tokenizzatore, usando in questo caso una versione pre-addestrata e fine tuned, su testi in italiano, di `BERT`, che è un transformer \"open\" (https://it.wikipedia.org/wiki/BERT) ed eseguibile anche localmente (alla prima esecuzione sarà dunque necessario attendere che il modello sia scaricato dal sito di Hugging Face: è un file di circa 400 MB che viene copiato in una cache locale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il tokenizzatore ha un vocabolario di 31102 token che riconosce.\n"
     ]
    }
   ],
   "source": [
    "from tokenizeutils import Model, colorize\n",
    "\n",
    "model = Model('dbmdz/bert-base-italian-xxl-cased', True)\n",
    "\n",
    "print(f\"Il tokenizzatore ha un vocabolario di {model.vocab_size} token che riconosce.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data una frase, chiediamo al tokenizzatore di trasformarla nella lista dei suoi token, ognuno dei quali corrispondente a un identificatore numerico univoco nel vocabolario.  \n",
    "Può accadere ovviamente che una parola non sia presente nel vocabolario, cioè non sia riconosciuta come un token: in tal caso il tokenizzatore la separa in componenti, ognuno dei quali sia invece un token (per rendere evidente questa separazione, il tokenizzatore aggiunge la stringa \"##\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Il testo 'Benvenuti a tutti voi.' \n",
      "è segmentato nella lista di token:\n",
      "['Benvenuti', 'a', 'tutti', 'voi', '.']\n",
      "corrispondente alla lista di identificatori numerici:\n",
      "[13790, 111, 570, 1411, 697]\n"
     ]
    }
   ],
   "source": [
    "text = \"Benvenuti a tutti voi.\"\n",
    "tokens = model.tokenizer.tokenize(text)\n",
    "token_ids = model.tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f\"\\nIl testo '{text}' \\nè segmentato nella lista di token:\")\n",
    "print(tokens)\n",
    "print(\"corrispondente alla lista di identificatori numerici:\")\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternando i colori, il risultato della tokenizzazione è forse ancora più chiaro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m Se\u001b[34m questo\u001b[31m mi\u001b[34m piace\u001b[31m sse\u001b[34m ,\u001b[31m lo\u001b[34m farei\u001b[31m ?\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(colorize(tokens, clean=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poiché la funzione che associa un identificatore numerico a ogni token è invertibile, è sempre possibile ricostruire la frase di partenza dalla successione degli identificatori numerici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ciao a tutti : ciao come state?\n"
     ]
    }
   ],
   "source": [
    "print(model.tokenizer.decode(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per curiosità, possiamo facilmente esplorare il contenuto del vocabolario, elencando alcuni dei token che contiene: è chiaro che non c'è una logica chiara nell'ordine con cui i token compaiono. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "possiamo GU ##7, testa mostra edi paragrafo capa € Ciao\n",
      "dispiace ##UR donna dovrebbe ##ata però ##ttutto ##ndo immag Sto\n"
     ]
    }
   ],
   "source": [
    "for i in range(2000, 2020):\n",
    "    print(model.tokenizer.decode([i]), end=('\\n' if i % 10 == 9 else ' '))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
