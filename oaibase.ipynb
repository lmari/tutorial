{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Il più semplice esempio di interazione con modelli locali via l'API di OpenAI\n",
    "\n",
    "Luca Mari, gennaio 2025  \n",
    "\n",
    "Quest'opera è distribuita con <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0\" target=\"_blank\">Licenza Creative Commons Attribuzione - Non commerciale - Condividi allo stesso modo 4.0 Internazionale</a>.  \n",
    "<img src=\"https://creativecommons.it/chapterIT/wp-content/uploads/2021/01/by-nc-sa.eu_.png\" width=\"100\">\n",
    "\n",
    "**Obiettivo**: comprendere le più semplice modalità di interazione con modelli locali via l'API di OpenAI.  \n",
    "**Precompetenze**: basi di Python.\n",
    "\n",
    "> Per eseguire questo notebook con VSCode sul proprio calcolatore, occorre:\n",
    "> * installare un interprete Python\n",
    "> * attivare un server locale che renda possibile l'interazione con un modello via l'API di OpenAI (per semplicità, supporremo che sia LM Studio, scaricabile da https://lmstudio.ai)\n",
    "> * scaricare da https://code.visualstudio.com/download e installare VSCode\n",
    "> * eseguire VSCode e attivare le estensioni per Python e Jupyter\n",
    "> * ancora in VSCode:\n",
    ">     * creare una cartella di lavoro e renderla la cartella corrente\n",
    ">     * copiare nella cartella il file di questa attività: [oaibase.ipynb](oaibase.ipynb)\n",
    ">     * aprire il notebook `oaibase.ipynb`\n",
    ">     * creare un ambiente virtuale locale Python (Select Kernel | Python Environments | Create Python Environment | Venv, e scegliere un interprete Python):\n",
    ">     * installare i moduli Python richiesti, eseguendo dal terminale:  \n",
    ">         `pip install openai`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importiamo i moduli Python necessari e specifichiamo l'_end point_ per l'accesso al server locale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pprint import pprint\n",
    "from IPython.display import Markdown, display\n",
    "import json\n",
    "\n",
    "def stream_print(response, max_length=100):\n",
    "    length = 0\n",
    "    for chunk in response:\n",
    "        text = chunk.choices[0].delta\n",
    "        if hasattr(text, 'content') and text.content:\n",
    "            print(text.content, end='', flush=True)\n",
    "            length += len(text.content)\n",
    "            if length > max_length:\n",
    "                print()\n",
    "                length = 0\n",
    "\n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\") # usa un server locale, per esempio con LM Studio  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elenco dei modelli disponibili\n",
    "Assumendo che il server sia attivo, questo è il più semplice esempio di una richiesta al server via l'API di OpenAI, per ottenere la lista dei modelli accessibili attraverso il server stesso (ma in effetti dunque anche per accertare che il server sia accessibile)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model(id='gemma-3-4b-it', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='gemma-3-27b-it', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='text-embedding-nomic-embed-text-v1.5@q4_k_m', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='qwen2.5-32b-instruct', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='qwq-32b', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='simplescaling_s1.1-32b', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='openthinker-32b', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='deepseek-r1-distill-qwen-32b', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='deepseek-r1-distill-qwen-1.5b', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='deepseek-r1-distill-llama-8b', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='deepseek-r1-distill-qwen-7b', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='phi-4', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='text-embedding-granite-embedding-278m-multilingual', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='llama-3.3-70b-instruct', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='qwen2.5-14b-instruct', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='llama-3.2-1b-instruct', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='llama-3.2-3b-instruct', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='mathstral-7b-v0.1', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='phi-3.5-mini-instruct', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='gemma-2-2b-it', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='meta-llama-3.1-8b-instruct', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='gemma-2-27b-it', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='llamantino-3-anita-8b-inst-dpo-ita_gguf', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='mistral-7b-instruct-v0.3', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='text-embedding-nomic-embed-text-v1.5@q8_0', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='text-embedding-nomic-embed-text-v1.5@f32', created=None, object='model', owned_by='organization_owner')]\n"
     ]
    }
   ],
   "source": [
    "models = client.models.list()\n",
    "pprint(models.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generazione di sentence embedding\n",
    "Assumendo che sia stato caricato un modello di _embedding_ sul server, possiamo usarlo per fare _sentence embedding_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La frase è stata codificata in un vettore di 768 numeri;\n",
      "i primi 5 sono: [0.10395540297031403, 0.050489071756601334, -0.0963636264204979, 0.011676624417304993, -0.04801320657134056]\n"
     ]
    }
   ],
   "source": [
    "text = \"La bellezza salverà il mondo.\"\n",
    "\n",
    "response = client.embeddings.create(\n",
    "    model=\"...\",\n",
    "    input=[text]\n",
    ").data[0].embedding\n",
    "\n",
    "print(f'''La frase è stata codificata in un vettore di {len(response)} numeri;\n",
    "i primi 5 sono: {response[:5]}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completion\n",
    "Questo è il più semplice esempio di una richiesta al server via l'API di OpenAI (stiamo supponendo che il server sia attivo e sia stato caricato un modello)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'intero oggetto JSON di risposta:\n",
      "{'choices': [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Ciao! Sono Gemma, un modello linguistico di grandi dimensioni creato dal team di Google DeepMind. Sono un'intelligenza artificiale open-weights, il che significa che sono ampiamente disponibile al pubblico. Posso aiutarti con una varietà di compiti che riguardano il linguaggio, come rispondere a domande, generare testo creativo e tradurre lingue. \\n\\nCosa posso fare per te oggi?\\n\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))],\n",
      " 'created': 1741886738,\n",
      " 'id': 'chatcmpl-9rf1hsgh6ifyjroq5p5mm',\n",
      " 'model': 'gemma-3-4b-it',\n",
      " 'object': 'chat.completion',\n",
      " 'service_tier': None,\n",
      " 'stats': {},\n",
      " 'system_fingerprint': 'gemma-3-4b-it',\n",
      " 'usage': CompletionUsage(completion_tokens=84, prompt_tokens=23, total_tokens=107, completion_tokens_details=None, prompt_tokens_details=None)}\n",
      "\n",
      "Il messaggio generato:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Ciao! Sono Gemma, un modello linguistico di grandi dimensioni creato dal team di Google DeepMind. Sono un'intelligenza artificiale open-weights, il che significa che sono ampiamente disponibile al pubblico. Posso aiutarti con una varietà di compiti che riguardano il linguaggio, come rispondere a domande, generare testo creativo e tradurre lingue. \n",
       "\n",
       "Cosa posso fare per te oggi?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Presentati: chi sei?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"...\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Rispondi sempre in italiano.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    max_tokens=-1,\n",
    "    temperature=0.7,\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(\"L'intero oggetto JSON di risposta:\")\n",
    "pprint(dict(response))\n",
    "\n",
    "print(\"\\nIl messaggio generato:\")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completion con stream dei token\n",
    "Quasi identica all'esempio precedente è la richiesta di una risposta che sia inviata un token per volta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ciao! Sono Gemma, un modello linguistico di grandi dimensioni creato dal team di Google DeepMind. Sono\n",
      " un'intelligenza artificiale progettata per rispondere alle tue domande e generare testo creativo. Sono\n",
      " un modello a \"pesi aperti\", quindi sono disponibile pubblicamente. \n",
      "\n",
      "Cosa ti incuriosisce?\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Presentati: chi sei?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"...\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Rispondi sempre in italiano.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    max_tokens=-1,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "stream_print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completion con structured output\n",
    "Solo un poco più complessa è una richiesta in cui si specifica lo schema JSON che vogliamo sia utilizzato nella risposta, nuovamente in accordo all'API di OpenAI, nella specifica _structured output_ (https://platform.openai.com/docs/guides/structured-outputs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"nome\": \"Gemini\",\n",
      "  \"produttore\": \"Google\",\n",
      "  \"caratteristiche principali\": \"Sono un modello lingu\n",
      "istico di grandi dimensioni, addestrato da Google. Posso comunicare e generare testo simile a quello umano\n",
      " in risposta a una vasta gamma di prompt e domande. Ad esempio, posso fornire riassunti di argomenti fatt\n",
      "uali o creare storie.\"\n",
      "}\n",
      " \t \t \t \t \t \t \t \t \t \t"
     ]
    }
   ],
   "source": [
    "response_format = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"presentazione\",\n",
    "        \"strict\": \"true\",\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"nome\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Il tuo nome\"\n",
    "                },\n",
    "                \"produttore\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Il nome del tuo produttore\",\n",
    "                    \"enum\": [\"OpenAI\", \"Google\", \"Meta\", \"other\"]\n",
    "                },\n",
    "                \"caratteristiche principali\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Le tue caratteristiche principali\"\n",
    "                },\n",
    "                \"caratteristiche secondarie\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Le tue caratteristiche secondarie (opzionale)\"\n",
    "                }\n",
    "            },\n",
    "        \"required\": [\"nome\", \"produttore\", \"caratteristiche principali\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "prompt = \"Presentati: chi sei?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"...\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Rispondi sempre in italiano.\" },\n",
    "        {\"role\": \"user\", \"content\": prompt }\n",
    "    ],\n",
    "    max_tokens=-1,\n",
    "    temperature=0.7,\n",
    "    stream=True,\n",
    "    response_format=response_format # type: ignore\n",
    ")\n",
    "\n",
    "stream_print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function calling\n",
    "\n",
    "https://platform.openai.com/docs/guides/function-calling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sqrt(number: float) -> float:   # la funzione da chiamare\n",
    "    return number ** 0.5\n",
    "\n",
    "tools = [                                   # la definizione della funzione\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"compute_sqrt\",\n",
    "            \"description\": \"Calcola la radice quadrata di un numero\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"number\": {\n",
    "                        \"type\": \"float\",\n",
    "                        \"description\": \"Il numero di cui calcolare la radice quadrata\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"number\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt = \"Calcola la radice quadrata di 64.\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"...\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Rispondi sempre in italiano.\" },\n",
    "        {\"role\": \"user\", \"content\": prompt }\n",
    "    ],\n",
    "    max_tokens=-1,\n",
    "    temperature=0.7,\n",
    "    tools=tools # type: ignore\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'intero oggetto JSON di risposta:\n",
      "{'choices': [Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='891526812', function=Function(arguments='{\"number\":64}', name='compute_sqrt'), type='function')]))],\n",
      " 'created': 1741888794,\n",
      " 'id': 'chatcmpl-umb4jvw0s4n3u00801kq',\n",
      " 'model': 'gemma-3-4b-it',\n",
      " 'object': 'chat.completion',\n",
      " 'service_tier': None,\n",
      " 'stats': {},\n",
      " 'system_fingerprint': 'gemma-3-4b-it',\n",
      " 'usage': CompletionUsage(completion_tokens=35, prompt_tokens=423, total_tokens=458, completion_tokens_details=None, prompt_tokens_details=None)}\n",
      "\n",
      "Il messaggio generato:\n",
      "ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='891526812', function=Function(arguments='{\"number\":64}', name='compute_sqrt'), type='function')])\n",
      "\n",
      "La parte del messaggio riferita alla funzione da chiamare:\n",
      "Function(arguments='{\"number\":64}', name='compute_sqrt')\n",
      "\n",
      "La funzione da chiamare è: compute_sqrt; i suoi argomenti sono: {'number': 64}\n"
     ]
    }
   ],
   "source": [
    "print(\"L'intero oggetto JSON di risposta:\")\n",
    "pprint(dict(response))\n",
    "\n",
    "print(\"\\nIl messaggio generato:\")\n",
    "pprint(response.choices[0].message)\n",
    "\n",
    "if response.choices[0].message.tool_calls:\n",
    "    tool_call = response.choices[0].message.tool_calls[0]\n",
    "\n",
    "    print(\"\\nLa parte del messaggio riferita alla funzione da chiamare:\")\n",
    "    print(tool_call.function)\n",
    "\n",
    "    function_name = tool_call.function.name\n",
    "    function_arguments = json.loads(tool_call.function.arguments)\n",
    "    print(f\"\\nLa funzione da chiamare è: {function_name}; i suoi argomenti sono: {function_arguments}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Il risultato della funzione è: 8.0\n"
     ]
    }
   ],
   "source": [
    "to_evaluate = function_name + \"(\" + str(function_arguments['number']) + \")\"\n",
    "result = eval(to_evaluate)\n",
    "\n",
    "print(f\"\\nIl risultato della funzione è: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
