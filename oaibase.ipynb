{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Il più semplice esempio di interazione con modelli locali via l'API di OpenAI\n",
    "\n",
    "Luca Mari, marzo 2025  \n",
    "\n",
    "Quest'opera è distribuita con <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0\" target=\"_blank\">Licenza Creative Commons Attribuzione - Non commerciale - Condividi allo stesso modo 4.0 Internazionale</a>.  \n",
    "<img src=\"https://creativecommons.it/chapterIT/wp-content/uploads/2021/01/by-nc-sa.eu_.png\" width=\"100\">\n",
    "\n",
    "**Obiettivo**: comprendere le più semplice modalità di interazione con modelli locali via l'API di OpenAI.  \n",
    "**Precompetenze**: basi di Python.\n",
    "\n",
    "> Per eseguire questo notebook con VSCode sul proprio calcolatore, occorre:\n",
    "> * installare un interprete Python\n",
    "> * attivare un server locale che renda possibile l'interazione con un modello via l'API di OpenAI (per semplicità, supporremo che sia LM Studio, scaricabile da https://lmstudio.ai)\n",
    "> * scaricare da https://code.visualstudio.com/download e installare VSCode\n",
    "> * eseguire VSCode e attivare le estensioni per Python e Jupyter\n",
    "> * ancora in VSCode:\n",
    ">     * creare una cartella di lavoro e renderla la cartella corrente\n",
    ">     * copiare nella cartella il file di questa attività: [oaibase.ipynb](oaibase.ipynb)\n",
    ">     * aprire il notebook `oaibase.ipynb`\n",
    ">     * creare un ambiente virtuale locale Python (Select Kernel | Python Environments | Create Python Environment | Venv, e scegliere un interprete Python):\n",
    ">     * installare i moduli Python richiesti, eseguendo dal terminale:  \n",
    ">         `pip install openai`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importiamo i moduli Python necessari, definiamo alcune funzioni di utilità, e specifichiamo l'_end point_ per l'accesso al server locale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pprint import pprint\n",
    "from IPython.display import Markdown, display\n",
    "import json\n",
    "\n",
    "def stream_print(response, max_length=100):\n",
    "    length = 0\n",
    "    for chunk in response:\n",
    "        text = chunk.choices[0].delta\n",
    "        if hasattr(text, 'content') and text.content:\n",
    "            print(text.content, end='', flush=True)\n",
    "            length += len(text.content)\n",
    "            if length > max_length:\n",
    "                print()\n",
    "                length = 0\n",
    "\n",
    "def stream_print_markdown(response):\n",
    "    buffer = \"\"\n",
    "    for chunk in response:\n",
    "        text = chunk.choices[0].delta\n",
    "        if hasattr(text, 'content') and text.content:\n",
    "            buffer += text.content\n",
    "            display(Markdown(buffer), clear=True)\n",
    "\n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\") # usa un server locale, per esempio con LM Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elenco dei modelli disponibili\n",
    "Assumendo che il server sia attivo, questo è il più semplice esempio di una richiesta al server via l'API di OpenAI, per ottenere la lista dei modelli accessibili attraverso il server stesso (ma in effetti dunque anche per accertare che il server sia accessibile)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model(id='gemma-3-4b-it', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='gemma-3-27b-it', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='text-embedding-nomic-embed-text-v1.5@q4_k_m', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='qwen2.5-32b-instruct', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='qwq-32b', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='simplescaling_s1.1-32b', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='openthinker-32b', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='deepseek-r1-distill-qwen-32b', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='deepseek-r1-distill-qwen-1.5b', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='deepseek-r1-distill-llama-8b', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='deepseek-r1-distill-qwen-7b', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='phi-4', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='text-embedding-granite-embedding-278m-multilingual', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='llama-3.3-70b-instruct', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='qwen2.5-14b-instruct', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='llama-3.2-1b-instruct', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='llama-3.2-3b-instruct', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='mathstral-7b-v0.1', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='phi-3.5-mini-instruct', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='meta-llama-3.1-8b-instruct', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='llamantino-3-anita-8b-inst-dpo-ita_gguf', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='mistral-7b-instruct-v0.3', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='text-embedding-nomic-embed-text-v1.5@q8_0', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='text-embedding-nomic-embed-text-v1.5@f32', created=None, object='model', owned_by='organization_owner')]\n"
     ]
    }
   ],
   "source": [
    "models = client.models.list()\n",
    "pprint(models.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generazione di sentence embedding\n",
    "Assumendo che sia stato caricato un modello di _embedding_ sul server, possiamo usarlo per fare _sentence embedding_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La frase è stata codificata in un vettore di 768 numeri;\n",
      "i primi 5 sono: [0.10395540297031403, 0.050489071756601334, -0.0963636264204979, 0.011676624417304993, -0.04801320657134056]\n"
     ]
    }
   ],
   "source": [
    "text = \"La bellezza salverà il mondo.\"\n",
    "\n",
    "response = client.embeddings.create(\n",
    "    model=\"...\",\n",
    "    input=[text]\n",
    ").data[0].embedding\n",
    "\n",
    "print(f'''La frase è stata codificata in un vettore di {len(response)} numeri;\n",
    "i primi 5 sono: {response[:5]}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completion\n",
    "Questo è il più semplice esempio di una richiesta al server via l'API di OpenAI (stiamo supponendo che il server sia attivo e sia stato caricato un modello)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'intero oggetto JSON di risposta:\n",
      "\n",
      "{'choices': [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Ciao! Sono Gemma, un modello linguistico di grandi dimensioni creato dal team di Google DeepMind. Sono un'intelligenza artificiale open-weights, il che significa che sono ampiamente disponibile al pubblico. Il mio obiettivo è assisterti con le tue domande e compiti nel miglior modo possibile. \\n\\nCosa posso fare per te oggi?\\n\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))],\n",
      " 'created': 1742481690,\n",
      " 'id': 'chatcmpl-wuqtppjn2ornxq21dgp0sr',\n",
      " 'model': 'gemma-3-4b-it',\n",
      " 'object': 'chat.completion',\n",
      " 'service_tier': None,\n",
      " 'stats': {},\n",
      " 'system_fingerprint': 'gemma-3-4b-it',\n",
      " 'usage': CompletionUsage(completion_tokens=73, prompt_tokens=23, total_tokens=96, completion_tokens_details=None, prompt_tokens_details=None)}\n",
      "\n",
      "Il messaggio generato:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Ciao! Sono Gemma, un modello linguistico di grandi dimensioni creato dal team di Google DeepMind. Sono un'intelligenza artificiale open-weights, il che significa che sono ampiamente disponibile al pubblico. Il mio obiettivo è assisterti con le tue domande e compiti nel miglior modo possibile. \n",
       "\n",
       "Cosa posso fare per te oggi?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Presentati: chi sei?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"...\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Rispondi sempre in italiano.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    max_tokens=-1,\n",
    "    temperature=2,\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(\"L'intero oggetto JSON di risposta:\\n\")\n",
    "pprint(dict(response))\n",
    "\n",
    "print(\"\\nIl messaggio generato:\")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completion con stream dei token\n",
    "Quasi identica all'esempio precedente è la richiesta di una risposta che sia inviata un token per volta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Ciao! Sono **un modello linguistico di grandi dimensioni**, addestrato da **Google**. In poche parole, sono un'intelligenza artificiale progettata per comprendere e generare testo simile a quello umano. \n",
       "\n",
       "Posso rispondere alle tue domande, tradurre lingue, scrivere diversi tipi di contenuti creativi e molto altro ancora. Sono ancora in fase di sviluppo, ma imparo continuamente nuove cose!\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Presentati: chi sei? (metti in grassetto le parole chiave)\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"...\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Rispondi sempre in italiano.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    max_tokens=-1,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "stream_print_markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completion con interpretazione del contenuto di un'immagine\n",
    "Se sul server è attivo un modello multimodale appropriato, la richiesta può includere l'URL di un'immagine e si può chiedere al modello di interpretare il contenuto dell'immagine stessa. In tal caso, per prima cosa occorre convertire l'immagine in formato base64.  \n",
    "Visualizziamo anche l'immagine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/aa/Fingandslide.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "from IPython.display import Image as IPImage\n",
    "\n",
    "url = \"https://upload.wikimedia.org/wikipedia/commons/a/aa/Fingandslide.jpg\"\n",
    "\n",
    "def get_image_and_convert_to_base64(url):\n",
    "    response = requests.get(url)\n",
    "    image = Image.open(BytesIO(response.content))\n",
    "    buffer = BytesIO()\n",
    "    image.save(buffer, format=\"PNG\")\n",
    "    return \"data:image/png;base64,\" + base64.b64encode(buffer.getvalue()).decode()\n",
    "\n",
    "image_base64 = get_image_and_convert_to_base64(url)\n",
    "\n",
    "IPImage(url=url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si può ora inviare la richiesta al modello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Certo, ecco una descrizione di quello che vedo nell'immagine:\n",
       "\n",
       "L'immagine mostra le mani di un chitarrista che suona una chitarra resofonica (spesso chiamata \"dobro\"). La chitarra ha un corpo metallico argentato con fori decorativi. Il manico della chitarra è in legno e presenta dei tasti. \n",
       "\n",
       "Il chitarrista sta usando le dita per pizzicare le corde, indossando dei plettri sulle dita. Sulla parte superiore del manico, si vede un oggetto cilindrico nero che probabilmente serve come slide (bottleneck) per produrre il caratteristico suono della resofonica.  Indossa pantaloni scuri.\n",
       "\n",
       "L'inquadratura è ravvicinata e focalizzata sulle mani e sulla chitarra, suggerendo l'attenzione alla tecnica esecutiva del musicista."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"...\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Rispondi sempre in italiano.\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"image_url\", \"image_url\": { \"url\": image_base64 }},\n",
    "            {\"type\": \"text\", \"text\": \"Descrivi cosa vedi\"}\n",
    "        ]}\n",
    "    ],\n",
    "    max_tokens=-1,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "stream_print_markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completion con structured output\n",
    "Solo un poco più complessa è una richiesta in cui si specifica lo schema JSON che vogliamo sia utilizzato nella risposta, nuovamente in accordo all'API di OpenAI, nella specifica _structured output_ (https://platform.openai.com/docs/guides/structured-outputs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"nome\": \"Gemini\",\n",
      "  \"produttore\": \"Google\",\n",
      "  \"caratteristiche principali\": \"Sono un modello lingu\n",
      "istico di grandi dimensioni, addestrato da Google. Posso comunicare e generare testo simile a quello umano\n",
      " in risposta a una vasta gamma di prompt e domande. Sono ancora in fase di sviluppo, ma ho imparato a\n",
      " scrivere diversi tipi di testo creativo, rispondere alle tue domande in modo informativo e completo,\n",
      " e seguire le tue istruzioni.\",\n",
      "  \"caratteristiche secondarie\": \"Posso tradurre lingue, riassumere testi\n",
      ", creare storie e molto altro. Sono progettato per essere utile e divertente!\"\n",
      "}\n",
      " \t \t \t \t \t \t \t \t \t \t\n"
     ]
    }
   ],
   "source": [
    "response_format = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"presentazione\",\n",
    "        \"strict\": \"true\",\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"nome\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Il tuo nome\"\n",
    "                },\n",
    "                \"produttore\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Il nome del tuo produttore\",\n",
    "                    \"enum\": [\"OpenAI\", \"Google\", \"Meta\", \"other\"]\n",
    "                },\n",
    "                \"caratteristiche principali\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Le tue caratteristiche principali\"\n",
    "                },\n",
    "                \"caratteristiche secondarie\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Le tue caratteristiche secondarie\"\n",
    "                }\n",
    "            },\n",
    "        \"required\": [\"nome\", \"produttore\", \"caratteristiche principali\", \"caratteristiche secondarie\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "prompt = \"Presentati: chi sei?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"...\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Rispondi sempre in italiano.\" },\n",
    "        {\"role\": \"user\", \"content\": prompt }\n",
    "    ],\n",
    "    max_tokens=-1,\n",
    "    temperature=0.7,\n",
    "    stream=True,\n",
    "    response_format=response_format # type: ignore\n",
    ")\n",
    "\n",
    "stream_print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function calling\n",
    "\n",
    "Ancora una volta in accordo all'API di OpenAI, un caso particolare di _structured output_ è il _function calling_ (https://platform.openai.com/docs/guides/function-calling), in cui lo schema JSON specifica una lista di funzioni, da trattare appunto come strumenti che possono essere usati.  \n",
    "Data la maggiore complessità di questo caso, sviluppiamo con qualche dettaglio un esempio.\n",
    "\n",
    "Supponendo di voler ottenere informazioni dal database della World Bank, e di volerlo fare con una chiamata alla sua API (qui qualche informazione al proposito: https://datahelpdesk.worldbank.org/knowledgebase/articles/898581-api-basic-call-structures), la struttura del sistema sarebbe come in questo diagramma:\n",
    "\n",
    "![schema](oaibase.drawio.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La richiesta: https://api.worldbank.org/v2/country/it/indicator/SP.URB.TOTL?date=2021&format=json\n",
      "\n",
      "La risposta:\n",
      "[{'lastupdated': '2025-01-28',\n",
      "  'page': 1,\n",
      "  'pages': 1,\n",
      "  'per_page': 50,\n",
      "  'sourceid': '2',\n",
      "  'total': 1},\n",
      " [{'country': {'id': 'IT', 'value': 'Italy'},\n",
      "   'countryiso3code': 'ITA',\n",
      "   'date': '2021',\n",
      "   'decimal': 0,\n",
      "   'indicator': {'id': 'SP.URB.TOTL', 'value': 'Urban population'},\n",
      "   'obs_status': '',\n",
      "   'unit': '',\n",
      "   'value': 42189154}]]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "request = \"https://api.worldbank.org/v2/country/it/indicator/SP.URB.TOTL?date=2021&format=json\"\n",
    "response = requests.get(request).json()\n",
    "\n",
    "print(f\"La richiesta: {request}\")\n",
    "print(\"\\nLa risposta:\")\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non è difficile elaborare il JSON della risposta per produrre un risultato più leggibile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paese: Italy\n",
      "Popolazione: 42189154\n"
     ]
    }
   ],
   "source": [
    "print(f\"Paese: {response[1][0]['country']['value']}\")\n",
    "print(f\"Popolazione: {response[1][0]['value']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ma rimane il fatto che la richiesta deve essere inviata rispettando il formato specificato nella API della World Bank.  \n",
    "Possiamo però usare un modello di linguaggio, a cui porre richieste in italiano, e facendo il modo che queste richieste vengano opportunamente tradotte e quindi eseguite come chiamate all'API, costruendo un sistema più complesso:\n",
    "\n",
    "![schema2](oaibase2.drawio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo strumento per inviare richieste all'API è in questo caso una semplice funzione Python, insieme con una sua descrizione JSON che consenta al modello di linguaggio di conoscere il nome della funzione e i suoi argomenti. In questo modo, l'oggetto JSON che il modello di linguaggio genera, in risposta a una richiesta che riceve, dovrebbe contenere l'informazione per eseguire la funzione, e quindi inviare all'API la richiesta nel formato opportuno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WB_API_call(countries: list[str], date: str) -> list:\n",
    "    result = []\n",
    "    for country in countries:\n",
    "        response = requests.get(f\"https://api.worldbank.org/v2/country/{country}/indicator/SP.URB.TOTL?date={date}&format=json\")\n",
    "        result.append(response.json())\n",
    "    return result\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"WB_API_call\",\n",
    "            \"description\": \"Ottieni informazioni dal database della World Bank via API\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"countries\": {\n",
    "                        \"type\": \"list\",\n",
    "                        \"description\": \"Il codice ISO dei Paesi di cui si vogliono ottenere i dati\"\n",
    "                    },\n",
    "                    \"date\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"L'anno di riferimento\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"countries\", \"date\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il modello di linguaggio opera perciò come traduttore, da richieste dell'utente formulate in italiano, a JSON per chiamare lo strumento (da cui il termine _function calling_). Vediamone un esempio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'choices': [Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='754505579', function=Function(arguments='{\"countries\":[\"IT\"],\"date\":\"2021\"}', name='WB_API_call'), type='function')]))],\n",
      " 'created': 1742481790,\n",
      " 'id': 'chatcmpl-zcsx8jpk0seeu5v8ud4vw6',\n",
      " 'model': 'gemma-3-4b-it',\n",
      " 'object': 'chat.completion',\n",
      " 'service_tier': None,\n",
      " 'stats': {},\n",
      " 'system_fingerprint': 'gemma-3-4b-it',\n",
      " 'usage': CompletionUsage(completion_tokens=43, prompt_tokens=473, total_tokens=516, completion_tokens_details=None, prompt_tokens_details=None)}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Ottieni dal database della World Bank i dati relativi alla popolazione urbana dell'Italia' nel 2021.\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"...\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Rispondi sempre in italiano.\" },\n",
    "        {\"role\": \"user\", \"content\": prompt }\n",
    "    ],\n",
    "    max_tokens=-1,\n",
    "    temperature=0.7,\n",
    "    tools=tools # type: ignore\n",
    ")\n",
    "\n",
    "pprint(dict(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dato che il formato di questo JSON è standard, la funzione per eseguire lo strumento specificato con gli argomenti specificati è generica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exec_tool(response, with_log=False):\n",
    "    if with_log:\n",
    "        print(\"L'intero oggetto JSON di risposta:\")\n",
    "        pprint(dict(response))\n",
    "        print(\"\\nIl messaggio generato:\")\n",
    "        pprint(response.choices[0].message)\n",
    "\n",
    "    if response.choices[0].message.tool_calls:\n",
    "        tool_call = response.choices[0].message.tool_calls[0]\n",
    "\n",
    "        if with_log:\n",
    "            print(\"\\nLa parte del messaggio riferita alla funzione da chiamare:\")\n",
    "            print(tool_call.function)\n",
    "\n",
    "        function_name = tool_call.function.name\n",
    "        function_arguments = json.loads(tool_call.function.arguments)\n",
    "        \n",
    "        if with_log:\n",
    "            pprint(f\"\\nLa funzione da chiamare è: {function_name}; i suoi argomenti sono: {function_arguments}\")\n",
    "\n",
    "        result = globals()[function_name](**function_arguments)\n",
    "\n",
    "        if with_log:\n",
    "            pprint(f\"\\nIl risultato della funzione è: {result}\")\n",
    "\n",
    "        return result\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ed ecco finalmente un esempio di _function calling_ completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'lastupdated': '2025-01-28',\n",
      "   'page': 1,\n",
      "   'pages': 1,\n",
      "   'per_page': 50,\n",
      "   'sourceid': '2',\n",
      "   'total': 1},\n",
      "  [{'country': {'id': 'BR', 'value': 'Brazil'},\n",
      "    'countryiso3code': 'BRA',\n",
      "    'date': '2019',\n",
      "    'decimal': 0,\n",
      "    'indicator': {'id': 'SP.URB.TOTL', 'value': 'Urban population'},\n",
      "    'obs_status': '',\n",
      "    'unit': '',\n",
      "    'value': 180121128}]],\n",
      " [{'lastupdated': '2025-01-28',\n",
      "   'page': 1,\n",
      "   'pages': 1,\n",
      "   'per_page': 50,\n",
      "   'sourceid': '2',\n",
      "   'total': 1},\n",
      "  [{'country': {'id': 'AR', 'value': 'Argentina'},\n",
      "    'countryiso3code': 'ARG',\n",
      "    'date': '2019',\n",
      "    'decimal': 0,\n",
      "    'indicator': {'id': 'SP.URB.TOTL', 'value': 'Urban population'},\n",
      "    'obs_status': '',\n",
      "    'unit': '',\n",
      "    'value': 41371540}]],\n",
      " [{'lastupdated': '2025-01-28',\n",
      "   'page': 1,\n",
      "   'pages': 1,\n",
      "   'per_page': 50,\n",
      "   'sourceid': '2',\n",
      "   'total': 1},\n",
      "  [{'country': {'id': 'CL', 'value': 'Chile'},\n",
      "    'countryiso3code': 'CHL',\n",
      "    'date': '2019',\n",
      "    'decimal': 0,\n",
      "    'indicator': {'id': 'SP.URB.TOTL', 'value': 'Urban population'},\n",
      "    'obs_status': '',\n",
      "    'unit': '',\n",
      "    'value': 16825479}]],\n",
      " [{'lastupdated': '2025-01-28',\n",
      "   'page': 1,\n",
      "   'pages': 1,\n",
      "   'per_page': 50,\n",
      "   'sourceid': '2',\n",
      "   'total': 1},\n",
      "  [{'country': {'id': 'PY', 'value': 'Paraguay'},\n",
      "    'countryiso3code': 'PRY',\n",
      "    'date': '2019',\n",
      "    'decimal': 0,\n",
      "    'indicator': {'id': 'SP.URB.TOTL', 'value': 'Urban population'},\n",
      "    'obs_status': '',\n",
      "    'unit': '',\n",
      "    'value': 4031453}]],\n",
      " [{'lastupdated': '2025-01-28',\n",
      "   'page': 1,\n",
      "   'pages': 1,\n",
      "   'per_page': 50,\n",
      "   'sourceid': '2',\n",
      "   'total': 1},\n",
      "  [{'country': {'id': 'VE', 'value': 'Venezuela, RB'},\n",
      "    'countryiso3code': 'VEN',\n",
      "    'date': '2019',\n",
      "    'decimal': 0,\n",
      "    'indicator': {'id': 'SP.URB.TOTL', 'value': 'Urban population'},\n",
      "    'obs_status': '',\n",
      "    'unit': '',\n",
      "    'value': 25534978}]],\n",
      " [{'lastupdated': '2025-01-28',\n",
      "   'page': 1,\n",
      "   'pages': 1,\n",
      "   'per_page': 50,\n",
      "   'sourceid': '2',\n",
      "   'total': 1},\n",
      "  [{'country': {'id': 'EC', 'value': 'Ecuador'},\n",
      "    'countryiso3code': 'ECU',\n",
      "    'date': '2019',\n",
      "    'decimal': 0,\n",
      "    'indicator': {'id': 'SP.URB.TOTL', 'value': 'Urban population'},\n",
      "    'obs_status': '',\n",
      "    'unit': '',\n",
      "    'value': 11095186}]],\n",
      " [{'lastupdated': '2025-01-28',\n",
      "   'page': 1,\n",
      "   'pages': 1,\n",
      "   'per_page': 50,\n",
      "   'sourceid': '2',\n",
      "   'total': 1},\n",
      "  [{'country': {'id': 'BO', 'value': 'Bolivia'},\n",
      "    'countryiso3code': 'BOL',\n",
      "    'date': '2019',\n",
      "    'decimal': 0,\n",
      "    'indicator': {'id': 'SP.URB.TOTL', 'value': 'Urban population'},\n",
      "    'obs_status': '',\n",
      "    'unit': '',\n",
      "    'value': 8143476}]]]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Ottieni dal database della World Bank i dati relativi alla popolazione urbana delle nazioni del Sud America nel 2019.\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"...\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Rispondi sempre in italiano.\" },\n",
    "        {\"role\": \"user\", \"content\": prompt }\n",
    "    ],\n",
    "    max_tokens=-1,\n",
    "    temperature=0.7,\n",
    "    tools=tools # type: ignore\n",
    ")\n",
    "\n",
    "final_response = exec_tool(response, with_log=True)\n",
    "pprint(final_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E la funzione che estrae i dati rilevanti dalla lista ottenuta. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'country': 'Brazil', 'population': 180121128},\n",
       " {'country': 'Argentina', 'population': 41371540},\n",
       " {'country': 'Chile', 'population': 16825479},\n",
       " {'country': 'Paraguay', 'population': 4031453},\n",
       " {'country': 'Venezuela, RB', 'population': 25534978},\n",
       " {'country': 'Ecuador', 'population': 11095186},\n",
       " {'country': 'Bolivia', 'population': 8143476}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def post_process_WB_API_call(response: list) -> list:\n",
    "    result = []\n",
    "    for country_response in response:\n",
    "        country = country_response[1][0]['country']['value']\n",
    "        population = country_response[1][0]['value']\n",
    "        result.append({\"country\": country, \"population\": population})\n",
    "    return result\n",
    "\n",
    "post_process_WB_API_call(final_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
