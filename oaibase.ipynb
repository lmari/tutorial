{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Il più semplice esempio di interazione con modelli locali via l'API di OpenAI\n",
    "\n",
    "Luca Mari, gennaio 2025  \n",
    "\n",
    "Quest'opera è distribuita con <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0\" target=\"_blank\">Licenza Creative Commons Attribuzione - Non commerciale - Condividi allo stesso modo 4.0 Internazionale</a>.  \n",
    "<img src=\"https://creativecommons.it/chapterIT/wp-content/uploads/2021/01/by-nc-sa.eu_.png\" width=\"100\">\n",
    "\n",
    "**Obiettivo**: comprendere le più semplice modalità di interazione con modelli locali via l'API di OpenAI.  \n",
    "**Precompetenze**: basi di Python.\n",
    "\n",
    "> Per eseguire questo notebook con VSCode sul proprio calcolatore, occorre:\n",
    "> * installare un interprete Python\n",
    "> * attivare un server locale che renda possibile l'interazione con un modello via l'API di OpenAI (per semplicità, supporremo che sia LM Studio, scaricabile da https://lmstudio.ai)\n",
    "> * scaricare da https://code.visualstudio.com/download e installare VSCode\n",
    "> * eseguire VSCode e attivare le estensioni per Python e Jupyter\n",
    "> * ancora in VSCode:\n",
    ">     * creare una cartella di lavoro e renderla la cartella corrente\n",
    ">     * copiare nella cartella il file di questa attività: [oaibase.ipynb](oaibase.ipynb)\n",
    ">     * aprire il notebook `oaibase.ipynb`\n",
    ">     * creare un ambiente virtuale locale Python (Select Kernel | Python Environments | Create Python Environment | Venv, e scegliere un interprete Python):\n",
    ">     * installare i moduli Python richiesti, eseguendo dal terminale:  \n",
    ">         `pip install openai`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importiamo i moduli Python necessari e specifichiamo l'_end point_ per l'accesso al server locale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pprint import pprint\n",
    "from IPython.display import Markdown, display\n",
    "import json\n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\") # usa un server locale, per esempio con LM Studio  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elenco dei modelli disponibili\n",
    "Assumendo che il server sia attivo, questo è il più semplice esempio di una richiesta al server via l'API di OpenAI, per ottenere la lista dei modelli accessibili attraverso il server stesso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model(id='qwen2.5-14b-instruct', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='deepseek-r1-distill-llama-8b', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='deepseek-r1-distill-qwen-7b', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='text-embedding-nomic-embed-text-v1.5@q4_k_m', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='phi-4', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='text-embedding-granite-embedding-278m-multilingual', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='llama-3.3-70b-instruct', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='llama-3.2-1b-instruct', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='llama-3.2-3b-instruct', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='mathstral-7b-v0.1', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='phi-3.5-mini-instruct', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='gemma-2-2b-it', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='meta-llama-3.1-8b-instruct', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='gemma-2-27b-it', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='llamantino-3-anita-8b-inst-dpo-ita_gguf', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='mistral-7b-instruct-v0.3', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='text-embedding-nomic-embed-text-v1.5@q8_0', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='text-embedding-nomic-embed-text-v1.5@f32', created=None, object='model', owned_by='organization_owner'),\n",
      " Model(id='llava-nousresearch_nous-hermes-2-vision', created=None, object='model', owned_by='organization_owner')]\n"
     ]
    }
   ],
   "source": [
    "models = client.models.list()\n",
    "pprint(models.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generazione di sentence embedding\n",
    "Assumendo che sia stato caricato un modello di _embedding_ sul server, possiamo usarlo per fare _sentence embedding_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La frase è stata codificata in un vettore di 768 numeri;\n",
      "i primi 5 sono: [-0.07184145599603653, -0.011897610500454903, -0.023683611303567886, -0.010468722321093082, 0.011160383932292461]\n"
     ]
    }
   ],
   "source": [
    "text = \"La bellezza salverà il mondo.\"\n",
    "\n",
    "response = client.embeddings.create(\n",
    "    model=\"...\",\n",
    "    input=[text]\n",
    ").data[0].embedding\n",
    "\n",
    "print(f'''La frase è stata codificata in un vettore di {len(response)} numeri;\n",
    "i primi 5 sono: {response[:5]}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completion\n",
    "Questo è il più semplice esempio di una richiesta al server via l'API di OpenAI (stiamo supponendo che il server sia attivo e sia stato caricato un modello)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'intero oggetto JSON di risposta:\n",
      "{'choices': [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Sono un'intelligenza artificiale creata da Alibaba Cloud per assistere, informare e conversare con gli utenti come te su una vasta gamma di argomenti. Mi chiamano Qwen, sono felice di incontrarti! Come posso aiutarti oggi?\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))],\n",
      " 'created': 1737802311,\n",
      " 'id': 'chatcmpl-bu85rmnqouedq3ww4j7nlg',\n",
      " 'model': 'qwen2.5-14b-instruct',\n",
      " 'object': 'chat.completion',\n",
      " 'service_tier': None,\n",
      " 'system_fingerprint': 'qwen2.5-14b-instruct',\n",
      " 'usage': CompletionUsage(completion_tokens=62, prompt_tokens=27, total_tokens=89, completion_tokens_details=None, prompt_tokens_details=None)}\n",
      "\n",
      "Il messaggio generato:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Sono un'intelligenza artificiale creata da Alibaba Cloud per assistere, informare e conversare con gli utenti come te su una vasta gamma di argomenti. Mi chiamano Qwen, sono felice di incontrarti! Come posso aiutarti oggi?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Presentati: chi sei?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"...\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Rispondi sempre in italiano.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    max_tokens=-1,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"L'intero oggetto JSON di risposta:\")\n",
    "pprint(dict(response))\n",
    "\n",
    "print(\"\\nIl messaggio generato:\")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completion con structured output\n",
    "Solo un poco più complessa è una richiesta in cui si specifica lo schema JSON che vogliamo sia utilizzato nella risposta, nuovamente in accordo all'API di OpenAI, nella specifica _structured output_ (https://platform.openai.com/docs/guides/structured-outputs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'intero oggetto JSON di risposta:\n",
      "{'choices': [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{ \"nome\": \"Phi\", \"caratteristiche principali\": \"Sono un modello linguistico di intelligenza artificiale sviluppato da Microsoft, progettato per comprendere e generare testo in modo coerente ed efficace. Le mie capacità includono la risposta a domande, la fornitura di spiegazioni su una varietà di argomenti, l\\'assistenza nella comprensione di concetti complessi e il supporto nell\\'apprendimento e nella risoluzione dei problemi. Sono in grado di offrire informazioni accurate fino alla mia ultima aggiornamento nel 2023. Tuttavia, è importante notare che non ho accesso a dati o eventi successivi a tale data.\" }', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))],\n",
      " 'created': 1737731736,\n",
      " 'id': 'chatcmpl-bpppqh3463njxp5njduom8',\n",
      " 'model': 'phi-4',\n",
      " 'object': 'chat.completion',\n",
      " 'service_tier': None,\n",
      " 'system_fingerprint': 'phi-4',\n",
      " 'usage': CompletionUsage(completion_tokens=168, prompt_tokens=25, total_tokens=193, completion_tokens_details=None, prompt_tokens_details=None)}\n",
      "\n",
      "Il messaggio generato:\n",
      "{'caratteristiche principali': 'Sono un modello linguistico di intelligenza '\n",
      "                               'artificiale sviluppato da Microsoft, '\n",
      "                               'progettato per comprendere e generare testo in '\n",
      "                               'modo coerente ed efficace. Le mie capacità '\n",
      "                               'includono la risposta a domande, la fornitura '\n",
      "                               'di spiegazioni su una varietà di argomenti, '\n",
      "                               \"l'assistenza nella comprensione di concetti \"\n",
      "                               \"complessi e il supporto nell'apprendimento e \"\n",
      "                               'nella risoluzione dei problemi. Sono in grado '\n",
      "                               'di offrire informazioni accurate fino alla mia '\n",
      "                               'ultima aggiornamento nel 2023. Tuttavia, è '\n",
      "                               'importante notare che non ho accesso a dati o '\n",
      "                               'eventi successivi a tale data.',\n",
      " 'nome': 'Phi'}\n"
     ]
    }
   ],
   "source": [
    "response_format = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"presentazione\",\n",
    "        \"strict\": \"true\",\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"nome\": {\"type\": \"string\"},\n",
    "                \"caratteristiche principali\": {\"type\": \"string\"},\n",
    "                \"caratteristiche secondarie\": {\"type\": \"string\"}\n",
    "            },\n",
    "        \"required\": [\"nome\", \"caratteristiche principali\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"...\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Rispondi sempre in italiano.\" },\n",
    "        {\"role\": \"user\", \"content\": \"Presentati: chi sei?\" }\n",
    "    ],\n",
    "    max_tokens=-1,\n",
    "    temperature=0.7,\n",
    "    response_format=response_format # type: ignore\n",
    ")\n",
    "\n",
    "print(\"L'intero oggetto JSON di risposta:\")\n",
    "pprint(dict(response))\n",
    "\n",
    "print(\"\\nIl messaggio generato:\")\n",
    "pprint(json.loads(response.choices[0].message.content)) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function calling\n",
    "\n",
    "https://platform.openai.com/docs/guides/function-calling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sqrt(number: float) -> float:   # la funzione da chiamare\n",
    "    return number ** 0.5\n",
    "\n",
    "tools = [                                   # la definizione della funzione\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"compute_sqrt\",\n",
    "            \"description\": \"Calcola la radice quadrata di un numero\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"number\": {\n",
    "                        \"type\": \"float\",\n",
    "                        \"description\": \"Il numero di cui calcolare la radice quadrata\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"number\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt = \"Calcola la radice quadrata di 64.\"\n",
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Rispondi sempre in italiano.\" },\n",
    "    {\"role\": \"user\", \"content\": prompt }\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"...\",\n",
    "    messages=messages, # type: ignore\n",
    "    max_tokens=-1,\n",
    "    temperature=0.7,\n",
    "    tools=tools # type: ignore\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'intero oggetto JSON di risposta:\n",
      "{'choices': [Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='534472074', function=Function(arguments='{\"number\":64}', name='compute_sqrt'), type='function')]))],\n",
      " 'created': 1736889005,\n",
      " 'id': 'chatcmpl-eaxj11o7e2mhrcb5qgiom',\n",
      " 'model': 'qwen2.5-14b-instruct',\n",
      " 'object': 'chat.completion',\n",
      " 'service_tier': None,\n",
      " 'system_fingerprint': 'qwen2.5-14b-instruct',\n",
      " 'usage': CompletionUsage(completion_tokens=22, prompt_tokens=188, total_tokens=210, completion_tokens_details=None, prompt_tokens_details=None)}\n",
      "\n",
      "Il messaggio generato:\n",
      "ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='534472074', function=Function(arguments='{\"number\":64}', name='compute_sqrt'), type='function')])\n",
      "\n",
      "La parte del messaggio riferita alla funzione da chiamare:\n",
      "Function(arguments='{\"number\":64}', name='compute_sqrt')\n",
      "\n",
      "La funzione da chiamare è: compute_sqrt; i suoi argomenti sono: {'number': 64}\n"
     ]
    }
   ],
   "source": [
    "print(\"L'intero oggetto JSON di risposta:\")\n",
    "pprint(dict(response))\n",
    "\n",
    "print(\"\\nIl messaggio generato:\")\n",
    "pprint(response.choices[0].message)\n",
    "\n",
    "if response.choices[0].message.tool_calls:\n",
    "    tool_call = response.choices[0].message.tool_calls[0]\n",
    "\n",
    "    print(\"\\nLa parte del messaggio riferita alla funzione da chiamare:\")\n",
    "    print(tool_call.function)\n",
    "\n",
    "    function_name = tool_call.function.name\n",
    "    function_arguments = json.loads(tool_call.function.arguments)\n",
    "    print(f\"\\nLa funzione da chiamare è: {function_name}; i suoi argomenti sono: {function_arguments}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Il risultato della funzione è: 8.0\n"
     ]
    }
   ],
   "source": [
    "to_evaluate = function_name + \"(\" + str(function_arguments['number']) + \")\"\n",
    "result = eval(to_evaluate)\n",
    "\n",
    "print(f\"\\nIl risultato della funzione è: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_2 = \"Quanto vale la radice quadrata di 64?\"\n",
    "messages.append({\"role\": \"user\", \"content\": prompt_2})\n",
    "messages.append({\"role\": \"assistant\", \"function_call\": {\"name\": function_name, \"arguments\": function_arguments}}) # type: ignore\n",
    "messages.append({\"role\": \"tool\", \"content\": str(result)})\n",
    "\n",
    "response_2 = client.chat.completions.create(\n",
    "    model=\"...\",\n",
    "    messages=messages, # type: ignore\n",
    "    tools=tools # type: ignore\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La radice quadrata di 64 è 8.0.\n"
     ]
    }
   ],
   "source": [
    "print(response_2.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
