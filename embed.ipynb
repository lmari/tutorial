{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un'esplorazione del *token embedding* con un transformer\n",
    "\n",
    "\n",
    "Luca Mari, gennaio 2025  \n",
    "\n",
    "Quest'opera è distribuita con <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0\" target=\"_blank\">Licenza Creative Commons Attribuzione - Non commerciale - Condividi allo stesso modo 4.0 Internazionale</a>.  \n",
    "<img src=\"https://creativecommons.it/chapterIT/wp-content/uploads/2021/01/by-nc-sa.eu_.png\" width=\"100\">\n",
    "\n",
    "**Obiettivo**: comprendere la logica della \"tokenizzazione\", il processo con cui un testo viene trasformato in una successione di elementi linguistici elementari (\"token\").  \n",
    "**Precompetenze**: basi di Python.\n",
    "\n",
    "> Per eseguire questo notebook, supponiamo con VSCode, occorre:\n",
    "> * installare un interprete Python\n",
    "> * scaricare da https://code.visualstudio.com/download e installare VSCode\n",
    "> * eseguire VSCode e attivare le estensioni per Python e Jupyter\n",
    "> * ancora in VSCode:\n",
    ">     * creare una cartella di lavoro e renderla la cartella corrente\n",
    ">     * copiare nella cartella i file di questa attività: [embed.ipynb](embed.ipynb), [tokenizeutils.py](tokenizeutils.py)]\n",
    ">     * aprire il notebook `embed.ipynb`\n",
    ">     * creare un ambiente virtuale locale Python (Select Kernel | Python Environments | Create Python Environment | Venv, e scegliere un interprete Python):\n",
    ">     * installare i moduli Python richiesti, eseguendo dal terminale:  \n",
    ">         `pip install torch transformers colorama python-docx`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per prima cosa, importiamo il modulo che contiene le funzioni per consentire un accesso \"di alto livello\" al modello pre-addestrato che opererà sia come _tokenizzatore_ sia come sistema di _embedding_, usando in questo caso una versione pre-addestrata e _fine tuned_, su testi in italiano, di `BERT`, che è un transformer accessibile liberamente (https://it.wikipedia.org/wiki/BERT) ed eseguibile anche localmente (alla prima esecuzione sarà dunque necessario attendere che il modello sia scaricato dal sito di Hugging Face: è un file di circa 400 MB che viene copiato in una cache locale) (non discutiamo qui di come questo modello sia stato addestrato a fare embedding).\n",
    "\n",
    "Dopo aver caricato il modello, verifichiamo che il processo sia andato a buon fine visualizzando le due informazioni principali:\n",
    "* il numero di token riconosciuti nel vocabolario del modello (`model.vocab_size`);\n",
    "* la dimensione del vettore in cui ogni token viene embedded (`model.embedding_dim`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il tokenizzatore ha un vocabolario di 31102 token che riconosce.\n",
      "Ogni token viene associato a ('embedded' in) un vettore di 768 numeri.\n"
     ]
    }
   ],
   "source": [
    "from tokenizeutils import Model\n",
    "from pprint import pprint\n",
    "\n",
    "model = Model('dbmdz/bert-base-italian-xxl-cased', True)\n",
    "\n",
    "print(f\"Il tokenizzatore ha un vocabolario di {model.vocab_size} token che riconosce.\")\n",
    "print(f\"Ogni token viene associato a ('embedded' in) un vettore di {model.embedding_dim} numeri.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il tokenizzatore mantiene un vocabolario dei token che riconosce, in una tabella in cui a ogni token è associato un identificatore univoco (`id`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il 'bellezza' ha l'identificatore 6108\n",
      "(ai token non presenti nel vocabolario è associato l'identificatore 101).\n"
     ]
    }
   ],
   "source": [
    "token = \"bellezza\"\n",
    "token_id = model.token_to_id(token)\n",
    "unknown_token_id = model.tokenizer.convert_tokens_to_ids(model.tokenizer.unk_token)\n",
    "\n",
    "if token_id is not unknown_token_id:\n",
    "    print(f\"Il '{token}' ha l'identificatore {token_id}\")\n",
    "else:\n",
    "    print(f\"Il '{token}' non è presente nel vocabolario.\")\n",
    "print(f\"(ai token non presenti nel vocabolario è associato l'identificatore {model.tokenizer.convert_tokens_to_ids(model.tokenizer.unk_token)}).\") # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il modello è stato addestrato a mappare (_to embed_, appunto) ogni token, con il suo identificatore, in una successione di numeri (c'è da considerare che i transformer, come `BERT`, operano sulla base di un embedding dinamico, in cui la successione di numeri associata a ogni token dipende anche dal contesto ('embedding posizionale'): qui noi lavoriamo solo con la componente statica del mapping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il token 'bellezza' è associato a un vettore di 768 elementi e i cui primi 5 elementi sono:\n",
      "[ 0.03051873  0.01173639 -0.04997671  0.0277972   0.02349026]\n"
     ]
    }
   ],
   "source": [
    "embedding = model.token_to_embedding(token)\n",
    "print(f\"Il token '{token}' è associato a un vettore di {len(embedding)} elementi e i cui primi 5 elementi sono:\\n{embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'embedding è dunque una funzione dall'insieme dei token riconosciuti, cioè il vocabolario, allo spazio metrico dei vettori a *n* dimensioni. Il modello che realizza tale funzione è addestrato in modo tale da cercare di indurre una struttura metrica sul vocabolario, sulla base del principio che token di significato simile dovrebbero essere associati a vettori vicini. Dato un token, è così possibile elencare i token che gli sono più simili nel vocabolario, cioè quelli che associati a vettori più vicini al vettore del token dato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I 10 token più simili a 'bellezza' nel vocabolario:\n",
      "[('bellezze', 0.45),\n",
      " ('dolcezza', 0.38),\n",
      " ('splendore', 0.38),\n",
      " ('estetica', 0.35),\n",
      " ('fascino', 0.34),\n",
      " ('Belle', 0.34),\n",
      " ('eleganza', 0.32),\n",
      " ('bellissima', 0.32),\n",
      " ('meraviglia', 0.31),\n",
      " ('bella', 0.31)]\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "print(f\"\\nI {n} token più simili a '{token}' nel vocabolario:\")\n",
    "pprint(model.most_similar(token, top_n=n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo calcolare la \"distanza semantica\" tra coppie di token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La distanza tra 'bellezza' e 'bellezza' è 0.00\n",
      "La distanza tra 'bellezza' e 'colore' è 1.51\n",
      "La distanza tra 'bellezza' e 'bianco' è 1.51\n",
      "La distanza tra 'colore' e 'bianco' è 1.35\n"
     ]
    }
   ],
   "source": [
    "token1 = \"bellezza\"\n",
    "token2 = \"colore\"\n",
    "token3 = \"bianco\"\n",
    "print(f\"La distanza tra '{token1}' e '{token1}' è {model.meaning_distance(token1, token1):.2f}\")\n",
    "print(f\"La distanza tra '{token1}' e '{token2}' è {model.meaning_distance(token1, token2):.2f}\")\n",
    "print(f\"La distanza tra '{token1}' e '{token3}' è {model.meaning_distance(token1, token3):.2f}\")\n",
    "print(f\"La distanza tra '{token2}' e '{token3}' è {model.meaning_distance(token2, token3):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'embedding consente di operare in modo piuttosto sofisticato sul vocabolario.  \n",
    "Per esempio, dati due token $A$ e $B$ a ognuno dei quali è stato associato un vettore, $v(A)$ e $v(B)$, se la regola di associazione $v(.)$ è sufficientemente ricca da un punto di vista semantico allora il vettore $v(A)-v(B)$ è associato alla relazione tra $A$ e $B$, interpretata dunque come la loro differenza.  \n",
    "In questo modo diventa possibile operare con _relazioni semantiche_ tra token. Per esempio, data la relazione tra $\"re\"$ e $\"uomo\"$, qual è il token $X$ che è nella stessa relazione di $\"re\"$ ma questa volta con $\"donna\"$? Questa domanda è dunque codificata come $v(\"re\")-v(\"uomo\")=v(X)-v(\"donna\")$, e perciò $v(X)=v(\"re\")+v(\"donna\")-v(\"uomo)$, in cui $\"re\"$ e $\"donna\"$ sono i \"casi positivi\" e $\"uomo\"$ è il \"caso negativo\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('regina', 0.31)]\n"
     ]
    }
   ],
   "source": [
    "positive_examples = [\"re\", \"donna\"]             # sovrano donna\n",
    "negative_examples = [\"uomo\"]\n",
    "pprint(model.combine_meanings(positive_examples, negative_examples, top_n=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In accordo a questo principio, possiamo sperimentare le capacità di _relazionalità semantica_ del modello che stiamo usando con alcuni altri esempi strutturalmente analoghi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Lisbona', 0.53), ('portoghesi', 0.48), ('portoghese', 0.45), ('Liverpool', 0.42), ('Barcellona', 0.4)]\n"
     ]
    }
   ],
   "source": [
    "positive_examples = [\"Roma\", \"Portogallo\"]         # capitale di uno stato\n",
    "negative_examples = [\"Italia\"]\n",
    "print(model.combine_meanings(positive_examples, negative_examples, top_n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Spagna', 0.39)]\n"
     ]
    }
   ],
   "source": [
    "positive_examples = [\"Italia\", \"Catalogna\"]     # stato di appartenenza di una regione\n",
    "negative_examples = [\"Lombardia\"]\n",
    "print(model.combine_meanings(positive_examples, negative_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Bonaparte', 0.44)]\n"
     ]
    }
   ],
   "source": [
    "positive_examples = [\"Garibaldi\", \"Francia\"]    # eroe nazionale\n",
    "negative_examples = [\"Italia\"]\n",
    "print(model.combine_meanings(positive_examples, negative_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('inverno', 0.5), ('autunno', 0.42), ('primavera', 0.34), ('estive', 0.32), ('estivo', 0.32)]\n"
     ]
    }
   ],
   "source": [
    "positive_examples = [\"estate\", \"freddo\"]        # stagione per temperatura \n",
    "negative_examples = [\"caldo\"]\n",
    "print(model.combine_meanings(positive_examples, negative_examples, top_n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('pianoforte', 0.63)]\n"
     ]
    }
   ],
   "source": [
    "positive_examples = [\"chitarra\", \"pianista\"]    # strumento di un musicista\n",
    "negative_examples = [\"chitarrista\"]\n",
    "print(model.combine_meanings(positive_examples, negative_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ginnastica', 0.4), ('allenamento', 0.4), ('atletica', 0.38), ('fitness', 0.37), ('pallavolo', 0.36)]\n"
     ]
    }
   ],
   "source": [
    "positive_examples = [\"nuoto\", \"palestra\"]       # sport praticato in un luogo\n",
    "negative_examples = [\"piscina\"]\n",
    "print(model.combine_meanings(positive_examples, negative_examples, top_n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('quattro', 0.56), ('cinque', 0.56), ('tre', 0.49), ('sette', 0.43), ('dodici', 0.41)]\n"
     ]
    }
   ],
   "source": [
    "positive_examples = [\"due\", \"dieci\"]              # numero successivo\n",
    "negative_examples = [\"uno\"]\n",
    "print(model.combine_meanings(positive_examples, negative_examples, top_n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('madre', 0.56)]\n"
     ]
    }
   ],
   "source": [
    "positive_examples = [\"padre\", \"figlia\"]         # genitore per genere\n",
    "negative_examples = [\"figlio\"]\n",
    "print(model.combine_meanings(positive_examples, negative_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('attrice', 0.63)]\n"
     ]
    }
   ],
   "source": [
    "positive_examples = [\"attore\", \"donna\"]         # femminile di un ruolo professionale\n",
    "negative_examples = [\"uomo\"]\n",
    "print(model.combine_meanings(positive_examples, negative_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('buono', 0.37)]\n"
     ]
    }
   ],
   "source": [
    "positive_examples = [\"bello\", \"cattivo\"]        # opposto di un aggettivo\n",
    "negative_examples = [\"brutto\"]\n",
    "print(model.combine_meanings(positive_examples, negative_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se i precedenti sono esempi ricchi semanticamente, proviamo a sperimentare anche con esempi solo grammaticali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('nera', 0.56)]\n"
     ]
    }
   ],
   "source": [
    "positive_examples = [\"bianca\", \"nero\"]          # femminile di un aggettivo\n",
    "negative_examples = [\"bianco\"]\n",
    "print(model.combine_meanings(positive_examples, negative_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('automobile', 0.54)]\n"
     ]
    }
   ],
   "source": [
    "positive_examples = [\"treno\", \"automobili\"]     # singolare di un sostantivo\n",
    "negative_examples = [\"treni\"]\n",
    "print(model.combine_meanings(positive_examples, negative_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('guardare', 0.57)]\n"
     ]
    }
   ],
   "source": [
    "positive_examples = [\"andare\", \"guardato\"]      # infinito di un verbo\n",
    "negative_examples = [\"andato\"]\n",
    "print(model.combine_meanings(positive_examples, negative_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ascoltando', 0.66)]\n"
     ]
    }
   ],
   "source": [
    "positive_examples = [\"pensando\", \"ascoltare\"]   # gerundio di un verbo\n",
    "negative_examples = [\"pensare\"]\n",
    "print(model.combine_meanings(positive_examples, negative_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo proseguire nell'esplorazione delle potenzialità di questa \"algebra semantica\", per esempio identificando i token al punto medio tra due token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gatti', 0.59),\n",
       " ('cani', 0.58),\n",
       " ('cucciolo', 0.46),\n",
       " ('animale', 0.43),\n",
       " ('animali', 0.4)]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token1 = \"cane\"\n",
    "token2 = \"gatto\"\n",
    "model.mean_meanings(token1, token2, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Europa', 0.53),\n",
       " ('Germania', 0.52),\n",
       " ('Spagna', 0.49),\n",
       " ('Inghilterra', 0.48),\n",
       " ('Olanda', 0.43)]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token1 = \"Italia\"\n",
    "token2 = \"Francia\"\n",
    "model.mean_meanings(token1, token2, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rosso', 0.6),\n",
       " ('grigio', 0.53),\n",
       " ('neri', 0.53),\n",
       " ('bianca', 0.5),\n",
       " ('nera', 0.5)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token1 = \"bianco\"\n",
    "token2 = \"nero\"\n",
    "model.mean_meanings(token1, token2, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"regina\"\n",
    "texts = [\"L'ultima regina di Inghilterra è stata Elisabetta II: ha regnato per molti anni.\",\n",
    "         \"L'ape regina è la femmina più grande di un alveare: ha un ruolo deteminante per il benessere dell'intero alveare.\",\n",
    "         \"La regina di picche è una carta da gioco: in molti giochi è più delle carte più potenti.\",]\n",
    "\n",
    "inputs, outputs = [], []\n",
    "for i in range(len(texts)):\n",
    "    inputs.append(model.tokenizer(texts[i], return_tensors=\"pt\"))\n",
    "    outputs.append(model.embedder(**inputs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['regina', 'Regina', 're', 'Queen', 'principessa', 'reale', 'regi', 'signora', 'sovrano', 'Elisabetta']\n",
      "['regina', 'Regina', 'domestica', 'tigre', 'bianca', 'femmina', 'nera', 'candida', 'maschio', 'rosa']\n",
      "['regina', 'Regina', 'testa', 'corona', 'principessa', 'risposta', 'coppia', 'coda', 'farina', 'casa']\n"
     ]
    }
   ],
   "source": [
    "import torch # type: ignore\n",
    "import torch.nn.functional as F # type: ignore\n",
    "\n",
    "seq_idxs, logits, probs, top_k_probs, top_k_token_ids, top5_tokens = [], [], [], [], [], []\n",
    "\n",
    "top_k = 10\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    seq_idxs.append(model.tokenizer.tokenize(texts[i]).index(token))\n",
    "\n",
    "    logits.append(outputs[i].logits[0, 1+seq_idxs[i], :])\n",
    "    probs.append(F.softmax(logits[i], dim=-1))\n",
    "    top_k_probs.append(torch.topk(probs[i], top_k).values.squeeze())\n",
    "    top_k_token_ids.append(torch.topk(probs[i], top_k).indices.squeeze())\n",
    "    top5_tokens.append([model.tokenizer.decode(token_id) for token_id in top_k_token_ids[i]])\n",
    "    print(top5_tokens[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
